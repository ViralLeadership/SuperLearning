{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Load the MovieData.csv dataset as described in this week's lesson, and use it to find the following values:\n",
    "\n",
    "a. What is the average US Gross of movies in the dataset?\n",
    "\n",
    "b. How many movies in the dataset have budgets greater than $20 million?\n",
    "\n",
    "c. How many movies were released by each film distributor? (Hint: this could be a good place to use dictionaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries needed\n",
    "import datetime as dt\n",
    "\n",
    "# Define the convert_to_int function\n",
    "def convert_to_int(text):\n",
    "    '''\n",
    "    Convert a string to an integer\n",
    "    '''\n",
    "    try:\n",
    "        return int(text)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Define the make_date function\n",
    "def make_date(date_str):\n",
    "    '''\n",
    "    Convert a MM/DD/YY string into a datetime object\n",
    "    '''\n",
    "    m, d, y = date_str.split(\"/\")\n",
    "    m = int(m)\n",
    "    d = int(d)\n",
    "    y = int(y)\n",
    "    if y > 13:\n",
    "        y += 1900\n",
    "    else:\n",
    "        y += 2000\n",
    "    return dt.datetime(y, m, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. What is the average US Gross of movies in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release_Date\tMovie\tDistributor\tBudget\tUS Gross\tWorldwide Gross\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Open the dataset\n",
    "f = open(\"MovieData.csv\")\n",
    "data = []\n",
    "print(f.readline()) # Skip the first row\n",
    "for row in f: \n",
    "    row =  row.split(\"\\t\")\n",
    "    row[0] = make_date(row[0])\n",
    "    row[3] = convert_to_int(row[3]) # Budget\n",
    "    row[4] = convert_to_int(row[4]) # US Gross\n",
    "    row[5] = convert_to_int(row[5]) # Worldwide Gross\n",
    "    data.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average US Gross of movies in the dataset is 44185793.44224795.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_gross = 0.0\n",
    "count = 0 \n",
    "for row in data:\n",
    "    try:\n",
    "        total_gross += row[4]\n",
    "        count += 1\n",
    "    except:\n",
    "        pass # Do nothing\n",
    "avg_US_gross = total_gross / count\n",
    "print(\"The average US Gross of movies in the dataset is \" + str(avg_US_gross) + \".\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. How many movies in the dataset have budgets greater than 20 million dollars?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of movies in the dataset with budgets greater than $20 million was 1657.\n"
     ]
    }
   ],
   "source": [
    "num_budgets_greater_20m = 0\n",
    "for row in data:\n",
    "    if row[3] > 20000000:\n",
    "        num_budgets_greater_20m += 1 \n",
    "print(\"The number of movies in the dataset with budgets greater than $20 million was %d.\" % num_budgets_greater_20m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. How many movies were released by each film distributor? (Hint: this could be a good place to use dictionaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 659\n",
      "Buena Vista 227\n",
      "New Line 138\n",
      "Sony 312\n",
      "Warner Bros. 311\n",
      "20th Century Fox 230\n",
      "Sony Pictures 2\n",
      "Paramount Pictures 258\n",
      "Universal 261\n",
      "Walt Disney Co. 9\n",
      "Columbia 26\n",
      "DreamWorks SKG 78\n",
      "MGM/UA 137\n",
      "Miramax 135\n",
      "Lionsgate 13\n",
      "Polygram Films 5\n",
      "Weinstein Co. 33\n",
      "United Artists 23\n",
      "Summit 11\n",
      "Lion's Gate 79\n",
      "TriStar Pictures 4\n",
      "DEJ Productions 1\n",
      "Warner Independent 3\n",
      "Weinstein 1\n",
      "Dimension Films 2\n",
      "Focus Features 36\n",
      "Third Rail 2\n",
      "Sony/Tristar 1\n",
      "Paramount Vantage 3\n",
      "Orion 17\n",
      "USA Films 15\n",
      "Dimension 28\n",
      "Sony Classics 78\n",
      "Magnolia 15\n",
      "Artisan 23\n",
      "Summit Entertainment 3\n",
      "MGM 3\n",
      "RS Entertainment 1\n",
      "CBS Films 3\n",
      "Freestyle 2\n",
      "Senator Films 1\n",
      "Fox Searchlight 65\n",
      "Warner Independent Pictures 7\n",
      "Sony/Gems 1\n",
      "Sony/Screen Gems 1\n",
      "Relativity 1\n",
      "New Market 7\n",
      "Rogue 1\n",
      "FilmDistrict 1\n",
      "ThinkFilm 9\n",
      "Gramercy 12\n",
      "Overture 6\n",
      "Savoy 3\n",
      "8 X Entertainment 1\n",
      "Fine Line 16\n",
      "Walt Disney Pictures 1\n",
      "Paramount Classics 12\n",
      "Destination 4\n",
      "Cloud Ten Pictures 1\n",
      "Filmways 2\n",
      "Weintraub 2\n",
      "IFC Films 20\n",
      "Galactic 1\n",
      "Film Foundry 1\n",
      "Samuel Goldwyn 9\n",
      "Picturehouse 6\n",
      "Alliance 4\n",
      "Disney 1\n",
      "Barking Cow 1\n",
      "Weinstein/Dimension 1\n",
      "Kino 5\n",
      "Destination Films 2\n",
      "Strand 8\n",
      "Newmarket Films 1\n",
      "Music Box 1\n",
      "First Look 9\n",
      "Consolidated Pictures Group 1\n",
      "October 6\n",
      "IDP 2\n",
      "Live Entertainment 3\n",
      "Screen Gems 1\n",
      "Embassy 1\n",
      "Cannon 4\n",
      "WinStar 1\n",
      "Goldwyn 6\n",
      "Black Diamond Pictures 1\n",
      "Eros 5\n",
      "First Independent Pictures 1\n",
      "Paladin 1\n",
      "WellSpring 2\n",
      "Roadside Attractions 4\n",
      "New Yorker 2\n",
      "Anchor Bay 4\n",
      "Universal/Rogue 1\n",
      "Apparition 4\n",
      "Strand Releasing 2\n",
      "Weinstein Ci. 1\n",
      "Cinema Service 1\n",
      "Trimark 9\n",
      "PION 1\n",
      "Wellspring 1\n",
      "Excel Entertainment 5\n",
      "Providence 2\n",
      "Giant 1\n",
      "Freestyle/Darko 1\n",
      "Roadside 2\n",
      "Indican 7\n",
      "Inerstar 1\n",
      "Yash Raj 1\n",
      "Avco Embassy 5\n",
      "Triumph 1\n",
      "Lorimar 2\n",
      "RKO 2\n",
      "Zeitgeist 7\n",
      "Oscilloscope 2\n",
      "Palm/Manga 1\n",
      "Lions Gate/IFC Films/Fellowship Adventure Group 1\n",
      "3D Entertainment 1\n",
      "Galaxy 1\n",
      "Cowboy 7\n",
      "October Films 1\n",
      "American International Pictures 1\n",
      "LIONS 1\n",
      "New Century Vista Film Company 1\n",
      "Hemdale Film Coorporation 1\n",
      "Tartan 1\n",
      "Palm Pictures 3\n",
      "Big Pictures 1\n",
      "IDP/Gold Circle 1\n",
      "Independent Artists 1\n",
      "Overture Films 1\n",
      "Empire 1\n",
      "Universal/Arenas Entertainment 1\n",
      "Phaedra 2\n",
      "New World 5\n",
      "CHRIST 1\n",
      "United Film Distribution 2\n",
      "Legacy 1\n",
      "Atlantic 1\n",
      "Regent Releasing 1\n",
      "Good Machine 1\n",
      "Attitude Films 1\n",
      "Odeon 1\n",
      "Shooting Gallery 1\n",
      "Small Planet 1\n",
      "Palisades Entertainment 1\n",
      "Access 1\n",
      "MORO 1\n",
      "Island/Alive 1\n",
      "Off Hollywood Pictures 1\n",
      "Artistic License 1\n",
      "IDP/Goldwyn 2\n",
      "Samuel Goldwyn Films 1\n",
      "Monterey Media 1\n",
      "New Films Int'l 1\n",
      "Bigger Picture 1\n",
      "NORTH 1\n",
      "Lions Gate 1\n",
      "David Keith Co. 1\n",
      "NEW LTN 1\n",
      "Roxie Releasing 1\n",
      "Videos 1\n",
      "Stratosphere 1\n",
      "Vitagraph Films 2\n",
      "Film Movement 2\n",
      "RBC Radio, LLC 1\n",
      "Romar 1\n",
      "Regent 1\n",
      "Rogue Pictures 1\n",
      "TLA Releasing 1\n",
      "Five and Two Pictures 1\n",
      "Rainforest Films 1\n",
      "Film Sales Co. 1\n",
      "Magnolia Pictures 1\n",
      "First Look Pictures 1\n",
      "Lot 47 1\n",
      "RED HOR 1\n",
      "The Movie Partners 1\n",
      "OpenEdge Media 1\n",
      "Mulberry Square Releasing 1\n",
      "J.F. Prods 1\n",
      "Halestorm 1\n",
      "Halestorm Entertainment 1\n",
      "CFP 1\n",
      "Fabrication Films 1\n",
      "New World Pictures 1\n",
      "Zion 1\n",
      "ART 1\n",
      "Painted Zebra Releasing 1\n",
      "Shotwell Media 1\n",
      "Outrider Pictures 1\n",
      "RAIN 1\n",
      "Island 1\n",
      "Testimony Pictures 1\n",
      "INWOO 1\n",
      "Off-Hollywood Distribution 1\n",
      "Jerry Gross Organization 1\n",
      "Avatar 1\n",
      "IDP/Sam Goldwyn 1\n",
      "Power Point 1\n",
      "IDP/Stratosphere 1\n",
      "Damiano 1\n",
      "Orion Classics 1\n",
      "JeTi Films 1\n",
      "Lavender House 1\n",
      "Cinema con Sabor 1\n",
      "Winstar 1\n",
      "Truly Indie 1\n",
      "CustomFlix 1\n"
     ]
    }
   ],
   "source": [
    "distributors = {}\n",
    "for row in data:\n",
    "    distributor = row[2]\n",
    "    # Verify whether this distributor already in dictionary and add to count if already there\n",
    "    if distributor in distributors:\n",
    "        distributors[distributor] += 1 \n",
    "    # Start new count for new distributor\n",
    "    else:\n",
    "        distributors[distributor] = 1 \n",
    "\n",
    "# Printing the results\n",
    "for distributor in distributors:\n",
    "    print(distributor, distributors[distributor])\n",
    "    # NOTE, there are some movies that did not come up under a distributor that I assume had Unknown or no distributor provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "Load the earthquake data in QuakeData.csv, and use it to answer the following questions:\n",
    "\n",
    "a. How many earthquakes are in the dataset?\n",
    "\n",
    "b. What is the average magnitude of earthquakes in the dataset?\n",
    "\n",
    "c. The DateTime format in this dataset is a bit trickier than it was for movies. Try to parse it into a datetime object. Do most earthquakes happen between midnight and noon, or noon to midnight?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### My setup items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DateTime,Latitude,Longitude,Depth,Magnitude,MagType,NbStations,Gap,Distance,RMS,Source,EventID,Version\n",
      "\n",
      "2012-01-01T00:30:08.770+00:00,12.008,143.487,35.0,5.1,mb,178,45,,1.20,pde,pde20120101003008770_35,1363392487731\n",
      "\n",
      "2012-01-01T00:43:42.770+00:00,12.014,143.536,35.0,4.4,mb,29,121,,0.98,pde,pde20120101004342770_35,1363392488431\n",
      "\n",
      "2012-01-01T00:50:08.040+00:00,-11.366,166.218,67.5,5.3,mb,143,43,,0.82,pde,pde20120101005008040_67,1363392488479\n",
      "\n",
      "2012-01-01T01:22:07.660+00:00,-6.747,130.008,145.0,4.2,mb,14,112,,1.16,pde,pde20120101012207660_145,1363392488594\n",
      "\n",
      "2012-01-01T02:35:21.110+00:00,23.472,91.834,27.8,4.6,mb,74,77,,0.65,pde,pde20120101023521110_27,1363392488611\n",
      "\n",
      "2012-01-01T02:40:36.400+00:00,6.677,-73.110,158.1,4.0,mb,23,129,,0.90,pde,pde20120101024036400_158,1363392488809\n",
      "\n",
      "2012-01-01T02:45:34.320+00:00,41.755,144.535,16.5,4.3,mb,44,135,,0.65,pde,pde20120101024534320_16,1363392488906\n",
      "\n",
      "2012-01-01T03:24:43.920+00:00,-3.207,137.723,51.4,4.1,mb,11,165,,1.24,pde,pde20120101032443920_51,1363392488962\n",
      "\n",
      "2012-01-01T04:10:46.710+00:00,-22.187,170.147,50.7,4.0,mb,12,183,,1.03,pde,pde20120101041046710_50,1363392489036\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = open(\"QuakeData.csv\")\n",
    "for i in range(10):\n",
    "    print(f.readline())\n",
    "    \n",
    "# Since each line represents data for an individual earthquake I define a function to convert earthquake_data\n",
    "def convert(earthquake_data):\n",
    "    try:\n",
    "        return float(earthquake_data)\n",
    "    except:\n",
    "        return earthquake_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DateTime', 'Latitude', 'Longitude', 'Depth', 'Magnitude', 'MagType', 'NbStations', 'Gap', 'Distance', 'RMS', 'Source', 'EventID', 'Version\\n']\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "f = open(\"QuakeData.csv\")\n",
    "print(f.readline().split(\",\")) # Look at the column headers\n",
    "for row in f:\n",
    "    row = row.split(\",\") # Split on commas\n",
    "    row = [convert(x) for x in row]\n",
    "    data.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. How many earthquakes are in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12684 earthquakes in the dataset.\n"
     ]
    }
   ],
   "source": [
    "num_earthquakes = len(data)\n",
    "print(\"There are %d earthquakes in the dataset.\" % num_earthquakes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. What is the average magnitude of earthquakes in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average magnitude of the earthquakes in the dataset is 4.558483128350625.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "quake_magnitudes = [row[4] for row in data]\n",
    "total_quake_magnitudes = sum(quake_magnitudes) * 1.0\n",
    "avg_quake_magnitude = total_quake_magnitudes / len(quake_magnitudes)\n",
    "print(\"The average magnitude of the earthquakes in the dataset is \" + str(avg_quake_magnitude) + \".\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Do most earthquakes happen between midnight and noon, or noon to midnight?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012-01-07T07:31:42.700+00:00\n",
      "2012-01-07T08:00:36.000+00:00\n",
      "2012-01-07T08:17:58.130+00:00\n",
      "2012-01-07T09:14:37.440+00:00\n",
      "2012-01-07T09:38:50.770+00:00\n"
     ]
    }
   ],
   "source": [
    "# looking at a few lines of the records\n",
    "for i in range(201, 206):\n",
    "    print(data[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2012-01-07T07:31:42.700+00:00', 1.705, 127.475, 114.8] ['mww', 299.0] ['', 0.76] ['pde20120107073142700_114', 1363392498664.0] [] []\n",
      "['2012-01-07T08:00:36.000+00:00', -17.774, -173.325, 35.0] ['mb', 51.0] ['', 0.97] ['pde20120107080036000_35', 1363392498944.0] [] []\n",
      "['2012-01-07T08:17:58.130+00:00', 37.443, 141.894, 57.9] ['mb', 72.0] ['', 0.77] ['pde20120107081758130_57', 1363392499021.0] [] []\n",
      "['2012-01-07T09:14:37.440+00:00', 4.555, 96.407, 40.0] ['mb', 33.0] ['', 0.87] ['pde20120107091437440_40', 1363392499031.0] [] []\n",
      "['2012-01-07T09:38:50.770+00:00', -7.143, 129.748, 103.0] ['mb', 9.0] ['', 1.06] ['pde20120107093850770_103', 1363392499034.0] [] []\n"
     ]
    }
   ],
   "source": [
    "for i in range(201, 206):\n",
    "    year = data[i][:4]\n",
    "    month = data[i][5:7]\n",
    "    day = data[i][8:10]\n",
    "    hour = data[i][11:13]\n",
    "    minute = data[i][14:16]\n",
    "    second = data[i][17:19]\n",
    "    \n",
    "    print(year, month, day, hour, minute, second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Confirming types of my variables\n",
    "print(type(year))\n",
    "print(type(month))\n",
    "print(type(day))\n",
    "print(type(hour))\n",
    "print(type(minute))\n",
    "print(type(second))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012-01-01T00:30:08.770+00:00\n",
      "2012 01 01 00 30 08\n"
     ]
    }
   ],
   "source": [
    "# just looking at the first earthquake in the dataset\n",
    "earthquake1 = data[0][0]\n",
    "print(earthquake1)\n",
    "year = earthquake1[:4]\n",
    "month = earthquake1[5:7]\n",
    "day = earthquake1[8:10]\n",
    "hour = earthquake1[11:13]\n",
    "minute = earthquake1[14:16]\n",
    "second = earthquake1[17:19]\n",
    "print(year, month, day, hour, minute, second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_datetime(earthquake_data):\n",
    "    year = int(earthquake_data[:4])\n",
    "    month = int(earthquake_data[5:7])\n",
    "    day = int(earthquake_data[8:10])\n",
    "    hour = int(earthquake_data[11:13])\n",
    "    minute = int(earthquake_data[14:16])\n",
    "    second = int(earthquake_data[17:19])\n",
    "    return dt.datetime(year, month, day, hour, minute, second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012-01-01 00:30:08\n"
     ]
    }
   ],
   "source": [
    "# Again looking at the first earthquake\n",
    "print(convert_datetime(earthquake1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I now have stripped out the time of quake for all 12684 earthquakes.\n",
      "The time of quake are stored in as: <class 'list'>.\n"
     ]
    }
   ],
   "source": [
    "times_of_quakes = [convert_datetime(row[0]) for row in data]\n",
    "print(\"I now have stripped out the time of quake for all %d earthquakes.\" % len(times_of_quakes))\n",
    "print(\"The time of quake are stored in as: %s.\" % type(times_of_quakes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 6241 earthquakes that occurred between midnight and noon.\n",
      "There were 6443 earthquakes that occurred between noon and midnight.\n",
      "There were 0 earthquakes that occurred exactly at noon.\n"
     ]
    }
   ],
   "source": [
    "num_before_noon = 0 # start counter for earthquakes between midnight and noon\n",
    "num_after_noon = 0 # start counter for earthquakes between noon and midnight\n",
    "num_exactly_noon = 0 # start counter for earthquakes that occurred exactly at noon\n",
    "for time_of_quake in times_of_quakes:\n",
    "    if time_of_quake.hour < 12:\n",
    "        num_before_noon += 1\n",
    "    elif time_of_quake == 12:\n",
    "        num_exactly_noon +=1\n",
    "    else:\n",
    "        num_after_noon += 1\n",
    "\n",
    "print(\"There were %d earthquakes that occurred between midnight and noon.\" % num_before_noon)\n",
    "print(\"There were %d earthquakes that occurred between noon and midnight.\" % num_after_noon)\n",
    "print(\"There were %d earthquakes that occurred exactly at noon.\" % num_exactly_noon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "PART 1: (2 points)\n",
    "\n",
    "In this question, you're asked to use what you've learned to analyze unstructured data -- a plain text file, data.txt containing the Facebook's description of its business in its mandatory annual report to shareholders (called a Form 10K).\n",
    "\n",
    "Bonus: sorted(...) is a built-in Python function for sorting lists according to some order. For extra credit, figure out how to sort dictionary keys based on the values in descending order, and sort all your answers below. To read more about sorting, start at the Python documentation here: https://docs.python.org/3.5/howto/sorting.html#sortinghowto\n",
    "\n",
    "a) Read in the text in the file, and count how many times each individual word appears. For this exercise, words are separated by whitespace -- don't worry about colons, dashes, etc.\n",
    "\n",
    "b) Stop words are words that appear so often in a language that they aren't useful for analysis (you may have noticed them in your results for a). Below is a list of stop words taken from NLTK, the Natural Language Toolkit for Python. Read in the document and count words again -- but this time, convert all the words to lower-case, and only include the words that aren't on the stop word list.\n",
    "\n",
    "PART 2: (3 points)\n",
    "\n",
    "Continue part bâ€¦ Also remove any punctuation (the characters .,?!-) from the beginning and end of words Finally, only output the words which appear more than once.\n",
    "\n",
    "c) Write the results of the previous section (including words that appear only once) to a csv file. There should be two columns, one for the word and the other for the count. For example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Read in the text in the file, and count how many times each individual word appears. For this exercise, words are separated by whitespace -- don't worry about colons, dashes, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overview 1\n",
      "Our 5\n",
      "mission 1\n",
      "is 8\n",
      "to 33\n",
      "give 1\n",
      "people 12\n",
      "the 12\n",
      "power 1\n",
      "share 4\n",
      "and 42\n",
      "make 2\n",
      "world 2\n",
      "more 1\n",
      "open 1\n",
      "connected. 1\n",
      "business 3\n",
      "focuses 2\n",
      "on 19\n",
      "creating 1\n",
      "value 2\n",
      "for 6\n",
      "people, 1\n",
      "marketers, 2\n",
      "developers. 2\n",
      "How 3\n",
      "We 10\n",
      "Create 3\n",
      "Value 3\n",
      "People 1\n",
      "Who 1\n",
      "Use 1\n",
      "Facebook 13\n",
      "top 1\n",
      "priority 1\n",
      "build 1\n",
      "useful 1\n",
      "engaging 1\n",
      "products 1\n",
      "that 6\n",
      "enable 5\n",
      "connect 1\n",
      "through 1\n",
      "mobile 13\n",
      "devices 3\n",
      "personal 6\n",
      "computers. 5\n",
      "also 5\n",
      "help 6\n",
      "discover 1\n",
      "learn 1\n",
      "about 1\n",
      "what 1\n",
      "going 1\n",
      "in 8\n",
      "around 1\n",
      "them, 1\n",
      "their 14\n",
      "opinions, 1\n",
      "ideas, 1\n",
      "photos 2\n",
      "videos, 2\n",
      "other 3\n",
      "activities 1\n",
      "with 8\n",
      "audiences 1\n",
      "ranging 1\n",
      "from 11\n",
      "closest 1\n",
      "friends 2\n",
      "public 1\n",
      "at 1\n",
      "large, 1\n",
      "stay 1\n",
      "connected 1\n",
      "everywhere 1\n",
      "by 4\n",
      "accessing 1\n",
      "our 7\n",
      "products, 1\n",
      "including: 1\n",
      "Facebook. 1\n",
      "The 1\n",
      "app 1\n",
      "website 1\n",
      "connect, 1\n",
      "share, 1\n",
      "discover, 1\n",
      "communicate 1\n",
      "each 2\n",
      "free 1\n",
      "available 3\n",
      "throughout 1\n",
      "world. 1\n",
      "had 2\n",
      "890 1\n",
      "million 2\n",
      "daily 1\n",
      "active 1\n",
      "users 1\n",
      "(DAUs) 1\n",
      "average 2\n",
      "December 4\n",
      "2014 2\n",
      ", 2\n",
      "an 5\n",
      "increase 3\n",
      "of 13\n",
      "18% 1\n",
      "compared 2\n",
      "2013 1\n",
      ". 1\n",
      "745 1\n",
      "DAUs 1\n",
      "who 6\n",
      "accessed 1\n",
      "a 9\n",
      "device 1\n",
      "34% 1\n",
      "2013. 1\n",
      "Instagram. 1\n",
      "Instagram 2\n",
      "application 7\n",
      "enables 2\n",
      "take 1\n",
      "or 5\n",
      "customize 1\n",
      "them 6\n",
      "filter 1\n",
      "effects, 1\n",
      "followers 1\n",
      "photo 1\n",
      "feed 1\n",
      "send 1\n",
      "directly 1\n",
      "friends. 1\n",
      "Messenger. 1\n",
      "Messenger 3\n",
      "mobile-to-mobile 1\n",
      "messaging 3\n",
      "Android, 2\n",
      "iOS 1\n",
      "Windows 2\n",
      "Phone 1\n",
      "devices. 3\n",
      "works 1\n",
      "similarly 1\n",
      "texting 1\n",
      "(SMS) 1\n",
      "online 4\n",
      "chat 1\n",
      "reach 2\n",
      "others 1\n",
      "instantly 1\n",
      "seamlessly 1\n",
      "integrates 1\n",
      "functionality 1\n",
      "WhatsApp. 1\n",
      "WhatsApp 1\n",
      "cross-platform 1\n",
      "allows 1\n",
      "exchange 1\n",
      "messages 1\n",
      "iOS, 1\n",
      "BlackBerry, 1\n",
      "Phone, 1\n",
      "Nokia 1\n",
      "Marketers 3\n",
      "providing 3\n",
      "all 1\n",
      "kinds 1\n",
      "including 3\n",
      "brand, 1\n",
      "direct 1\n",
      "response, 1\n",
      "small 1\n",
      "medium-sized 1\n",
      "businesses, 1\n",
      "achieve 1\n",
      "objectives, 1\n",
      "whether 1\n",
      "it 1\n",
      "driving 1\n",
      "sales, 2\n",
      "in-store 2\n",
      "awareness 1\n",
      "brand. 1\n",
      "generate 3\n",
      "substantial 1\n",
      "majority 1\n",
      "revenue 4\n",
      "selling 1\n",
      "advertising 1\n",
      "placements 1\n",
      "marketers. 1\n",
      "ads 9\n",
      "let 1\n",
      "marketers 4\n",
      "based 2\n",
      "variety 1\n",
      "factors 1\n",
      "age, 1\n",
      "gender, 1\n",
      "location, 1\n",
      "interests. 1\n",
      "purchase 2\n",
      "can 4\n",
      "appear 1\n",
      "multiple 1\n",
      "places 1\n",
      "News 1\n",
      "Feed 1\n",
      "computers, 1\n",
      "right-hand 1\n",
      "side 1\n",
      "ad 6\n",
      "planning 1\n",
      "tools 3\n",
      "are 2\n",
      "designed 1\n",
      "align 1\n",
      "marketers' 1\n",
      "goals. 1\n",
      "When 1\n",
      "create 2\n",
      "campaign 1\n",
      "Facebook, 2\n",
      "they 2\n",
      "specify 1\n",
      "budget, 1\n",
      "marketing 1\n",
      "objectives 1\n",
      "types 1\n",
      "want 1\n",
      "reach. 1\n",
      "Facebook's 1\n",
      "serving 1\n",
      "technology 1\n",
      "then 1\n",
      "dynamically 1\n",
      "determines 1\n",
      "best 1\n",
      "show 2\n",
      "person 1\n",
      "those 2\n",
      "dimensions. 1\n",
      "use 4\n",
      "platform's 1\n",
      "insights 2\n",
      "measure 1\n",
      "optimize 1\n",
      "both 1\n",
      "performance 1\n",
      "campaigns. 1\n",
      "These 1\n",
      "not 1\n",
      "only 1\n",
      "understand 1\n",
      "how 1\n",
      "drove 1\n",
      "results 1\n",
      "but 1\n",
      "modifications 1\n",
      "campaigns 1\n",
      "improve 1\n",
      "results. 1\n",
      "In 1\n",
      "addition 1\n",
      "buy 1\n",
      "websites 1\n",
      "applications 6\n",
      "such 3\n",
      "as 4\n",
      "Audience 3\n",
      "Network, 2\n",
      "Atlas, 1\n",
      "LiveRail. 1\n",
      "Developers 1\n",
      "supports 1\n",
      "developersb\u0000\u0019 1\n",
      "efforts 1\n",
      "build, 1\n",
      "grow, 1\n",
      "monetize 3\n",
      "web 4\n",
      "applications. 2\n",
      "First, 1\n",
      "we 4\n",
      "provide 1\n",
      "set 1\n",
      "development 1\n",
      "programming 1\n",
      "interfaces 1\n",
      "(APIs) 1\n",
      "developers 9\n",
      "easily 1\n",
      "integrate 1\n",
      "across 1\n",
      "platforms 1\n",
      "Second, 1\n",
      "grow 1\n",
      "tools, 1\n",
      "social 1\n",
      "plugins, 1\n",
      "exposure, 1\n",
      "distribution 1\n",
      "engagement 1\n",
      "By 1\n",
      "using 1\n",
      "sharing, 1\n",
      "messaging, 1\n",
      "invites, 1\n",
      "requests, 1\n",
      "ads, 1\n",
      "have 1\n",
      "number 1\n",
      "ways 1\n",
      "drive 1\n",
      "discovery 1\n",
      "user 1\n",
      "engagement. 1\n",
      "Finally, 1\n",
      "Payments 2\n",
      "infrastructure 2\n",
      "receive 2\n",
      "payments 1\n",
      "easy-to-use, 1\n",
      "secure, 1\n",
      "trusted 1\n",
      "environment, 1\n",
      "well 1\n",
      "where 1\n",
      "able 1\n",
      "showing 1\n",
      "advertisers 2\n",
      "within 2\n",
      "application. 1\n",
      "sell 1\n",
      "virtual 1\n",
      "digital 1\n",
      "goods 1\n",
      "choose 1\n",
      "us, 1\n",
      "portion 1\n",
      "Network. 1\n"
     ]
    }
   ],
   "source": [
    "f = open(\"data.txt\")\n",
    "word_counts = {}\n",
    "for row in f:\n",
    "    for word in row.split():\n",
    "        if word in word_counts:\n",
    "            word_counts[word] += 1\n",
    "        else:\n",
    "            word_counts[word] = 1\n",
    "f.close()\n",
    "\n",
    "for word in word_counts:\n",
    "    print(word, word_counts[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Stop words are words that appear so often in a language that they aren't useful for analysis (you may have noticed them in your results for a). Below is a list of stop words taken from NLTK, the Natural Language Toolkit for Python. Read in the document and count words again -- but this time, convert all the words to lower-case, and only include the words that aren't on the stop word list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', \n",
    "'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'overview': 1, 'mission': 1, 'give': 1, 'people': 14, 'power': 1, 'share': 5, 'make': 2, 'world': 3, 'open': 1, 'connected': 2, 'business': 3, 'focuses': 2, 'creating': 1, 'value': 5, 'marketers': 10, 'developers': 12, 'create': 5, 'use': 5, 'facebook': 16, 'top': 1, 'priority': 1, 'build': 2, 'useful': 1, 'engaging': 1, 'products': 2, 'enable': 5, 'connect': 2, 'mobile': 13, 'devices': 6, 'personal': 6, 'computers': 6, 'also': 5, 'help': 6, 'discover': 2, 'learn': 1, 'going': 1, 'around': 1, 'opinions': 1, 'ideas': 1, 'photos': 2, 'videos': 2, 'activities': 1, 'audiences': 1, 'ranging': 1, 'closest': 1, 'friends': 3, 'public': 1, 'large': 1, 'stay': 1, 'everywhere': 1, 'accessing': 1, 'including:': 1, 'app': 1, 'website': 1, 'communicate': 1, 'free': 1, 'available': 3, 'throughout': 1, '890': 1, 'million': 2, 'daily': 1, 'active': 1, 'users': 1, '(daus)': 1, 'average': 2, 'december': 4, '2014': 2, '': 3, 'increase': 3, '18%': 1, 'compared': 2, '2013': 2, '745': 1, 'daus': 1, 'accessed': 1, 'device': 1, '34%': 1, 'instagram': 3, 'application': 8, 'enables': 2, 'take': 1, 'customize': 1, 'filter': 1, 'effects': 1, 'followers': 1, 'photo': 1, 'feed': 2, 'send': 1, 'directly': 1, 'messenger': 4, 'mobile-to-mobile': 1, 'messaging': 4, 'android': 2, 'ios': 2, 'windows': 2, 'phone': 2, 'works': 1, 'similarly': 1, 'texting': 1, '(sms)': 1, 'online': 4, 'chat': 1, 'reach': 3, 'others': 1, 'instantly': 1, 'seamlessly': 1, 'integrates': 1, 'functionality': 1, 'whatsapp': 2, 'cross-platform': 1, 'allows': 1, 'exchange': 1, 'messages': 1, 'blackberry': 1, 'nokia': 1, 'providing': 3, 'kinds': 1, 'including': 3, 'brand': 2, 'direct': 1, 'response': 1, 'small': 1, 'medium-sized': 1, 'businesses': 1, 'achieve': 1, 'objectives': 2, 'whether': 1, 'driving': 1, 'sales': 2, 'in-store': 2, 'awareness': 1, 'generate': 3, 'substantial': 1, 'majority': 1, 'revenue': 4, 'selling': 1, 'advertising': 1, 'placements': 1, 'ads': 10, 'let': 1, 'based': 2, 'variety': 1, 'factors': 1, 'age': 1, 'gender': 1, 'location': 1, 'interests': 1, 'purchase': 2, 'appear': 1, 'multiple': 1, 'places': 1, 'news': 1, 'right-hand': 1, 'side': 1, 'ad': 6, 'planning': 1, 'tools': 4, 'designed': 1, 'align': 1, \"marketers'\": 1, 'goals': 1, 'campaign': 1, 'specify': 1, 'budget': 1, 'marketing': 1, 'types': 1, 'want': 1, \"facebook's\": 1, 'serving': 1, 'technology': 1, 'dynamically': 1, 'determines': 1, 'best': 1, 'show': 2, 'person': 1, 'dimensions': 1, \"platform's\": 1, 'insights': 2, 'measure': 1, 'optimize': 1, 'performance': 1, 'campaigns': 2, 'understand': 1, 'drove': 1, 'results': 2, 'modifications': 1, 'improve': 1, 'addition': 1, 'buy': 1, 'websites': 1, 'applications': 8, 'audience': 3, 'network': 3, 'atlas': 1, 'liverail': 1, 'supports': 1, 'developersb\\x00\\x19': 1, 'efforts': 1, 'grow': 2, 'monetize': 3, 'web': 4, 'first': 1, 'provide': 1, 'set': 1, 'development': 1, 'programming': 1, 'interfaces': 1, '(apis)': 1, 'easily': 1, 'integrate': 1, 'across': 1, 'platforms': 1, 'second': 1, 'social': 1, 'plugins': 1, 'exposure': 1, 'distribution': 1, 'engagement': 2, 'using': 1, 'sharing': 1, 'invites': 1, 'requests': 1, 'number': 1, 'ways': 1, 'drive': 1, 'discovery': 1, 'user': 1, 'finally': 1, 'payments': 3, 'infrastructure': 2, 'receive': 2, 'easy-to-use': 1, 'secure': 1, 'trusted': 1, 'environment': 1, 'well': 1, 'able': 1, 'showing': 1, 'advertisers': 2, 'within': 2, 'sell': 1, 'virtual': 1, 'digital': 1, 'goods': 1, 'choose': 1, 'us': 1, 'portion': 1}\n"
     ]
    }
   ],
   "source": [
    "f = open(\"data.txt\")\n",
    "word_counts = {}\n",
    "for row in f:\n",
    "    for word in row.split():\n",
    "        word = word.lower()\n",
    "        word = word.strip(\".,!?- \")\n",
    "        if word not in STOP_WORDS:\n",
    "            if word in word_counts:\n",
    "                word_counts[word] += 1\n",
    "            else:\n",
    "                word_counts[word] = 1\n",
    "f.close()\n",
    "print(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_sorted = sorted(word_counts, reverse=True)\n",
    "# I don't think this answers the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['world', 'works', 'within', 'windows', 'whether', 'whatsapp', 'well', 'websites', 'website', 'web', 'ways', 'want', 'virtual', 'videos', 'variety', 'value', 'using', 'users', 'user', 'useful', 'use', 'us', 'understand', 'types', 'trusted', 'top', 'tools', 'throughout', 'texting', 'technology', 'take', 'supports', 'substantial', 'stay', 'specify', 'social', 'small', 'similarly', 'side', 'showing', 'show', 'sharing', 'share', 'set', 'serving', 'send', 'selling', 'sell', 'secure', 'second', 'seamlessly', 'sales', 'right-hand', 'revenue', 'results', 'response', 'requests', 'receive', 'reach', 'ranging', 'purchase', 'public', 'providing', 'provide', 'programming', 'products', 'priority', 'power', 'portion', 'plugins', 'platforms', \"platform's\", 'planning', 'places', 'placements', 'photos', 'photo', 'phone', 'personal', 'person', 'performance', 'people', 'payments', 'overview', 'others', 'optimize', 'opinions', 'open', 'online', 'objectives', 'number', 'nokia', 'news', 'network', 'multiple', 'monetize', 'modifications', 'mobile-to-mobile', 'mobile', 'mission', 'million', 'messenger', 'messaging', 'messages', 'medium-sized', 'measure', 'marketing', \"marketers'\", 'marketers', 'make', 'majority', 'location', 'liverail', 'let', 'learn', 'large', 'kinds', 'ios', 'invites', 'interfaces', 'interests', 'integrates', 'integrate', 'instantly', 'instagram', 'insights', 'infrastructure', 'increase', 'including:', 'including', 'in-store', 'improve', 'ideas', 'help', 'grow', 'goods', 'going', 'goals', 'give', 'generate', 'gender', 'functionality', 'friends', 'free', 'followers', 'focuses', 'first', 'finally', 'filter', 'feed', 'factors', \"facebook's\", 'facebook', 'exposure', 'exchange', 'everywhere', 'environment', 'engaging', 'engagement', 'enables', 'enable', 'efforts', 'effects', 'easy-to-use', 'easily', 'dynamically', 'drove', 'driving', 'drive', 'distribution', 'discovery', 'discover', 'directly', 'direct', 'dimensions', 'digital', 'devices', 'device', 'development', 'developersb\\x00\\x19', 'developers', 'determines', 'designed', 'december', 'daus', 'daily', 'customize', 'cross-platform', 'creating', 'create', 'connected', 'connect', 'computers', 'compared', 'communicate', 'closest', 'choose', 'chat', 'campaigns', 'campaign', 'buy', 'businesses', 'business', 'build', 'budget', 'brand', 'blackberry', 'best', 'based', 'awareness', 'average', 'available', 'audiences', 'audience', 'atlas', 'around', 'applications', 'application', 'appear', 'app', 'android', 'also', 'allows', 'align', 'age', 'advertising', 'advertisers', 'ads', 'addition', 'ad', 'activities', 'active', 'across', 'achieve', 'accessing', 'accessed', 'able', '890', '745', '34%', '2014', '2013', '18%', '(sms)', '(daus)', '(apis)', '']\n"
     ]
    }
   ],
   "source": [
    "print(words_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Write the results of the previous section (including words that appear only once) to a csv file. There should be two columns, one for the word and the other for the count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"hw#2_question3c.csv\", \"w\")\n",
    "header = \"Word,Count\\n\"\n",
    "f.write(header)\n",
    "\n",
    "for word in words_sorted:\n",
    "    count = str(word_counts[word])\n",
    "    row = word + \",\" + count + \"\\n\"\n",
    "    f.write(row)\n",
    "\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
